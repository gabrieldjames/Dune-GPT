{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f290bce",
   "metadata": {},
   "source": [
    "## Recreating GPT Manually (Mostly)\n",
    "\n",
    "Personal project to recreate GPT architecture using PyTorch to generate Dune text on a character by character basis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb04f3",
   "metadata": {},
   "source": [
    "I utilized softmax, cross_entropy, Adam optimizer, and nn.Module class (for loss.backward()) from Pytorch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f12715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6a6c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1080\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "#verify GPU is in use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72c09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding Layer\n",
    "class Embedding():\n",
    "    \n",
    "    def __init__(self, embeddings_card, embedding_dimensions):\n",
    "        \n",
    "        self.weights = torch.randn((embeddings_card, embedding_dimensions)).cuda()\n",
    "        \n",
    "        \n",
    "    def __call__(self, encoding):\n",
    "        \n",
    "        #intended: one hot encoding\n",
    "        self.out = self.weights[encoding]\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    def params(self):\n",
    "        \n",
    "        return [self.weights]\n",
    "\n",
    "\n",
    "    \n",
    "#Simple linear layer\n",
    "class Linear():\n",
    "    \n",
    "    def __init__(self, input_card, output_card, bias=True):\n",
    "        \n",
    "        #kaiming initialization\n",
    "        self.weights = torch.randn(input_card, output_card).cuda()*(2/input_card)**.5\n",
    "        \n",
    "        if bias:\n",
    "            self.biases = torch.zeros(output_card).cuda()\n",
    "        else:\n",
    "            self.biases = None\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        if self.biases is None:\n",
    "            self.out = x@self.weights\n",
    "        else:\n",
    "            self.out = x@self.weights + self.biases\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    def params(self):\n",
    "        if self.biases is None:\n",
    "            return [self.weights]\n",
    "        else:\n",
    "            return [self.weights] + [self.biases]\n",
    "\n",
    "        \n",
    "\n",
    "#Layer Normalization\n",
    "class LayerNorm():\n",
    "    \n",
    "    def __init__(self, dimension, epsilon=1e-6):\n",
    "        \n",
    "        #learned weights to slide normalized distribution\n",
    "        self.gamma = torch.ones(dimension).cuda()\n",
    "        self.beta = torch.zeros(dimension).cuda()\n",
    "        \n",
    "        #handle 0 variance edge case\n",
    "        self.epsilon = epsilon \n",
    "    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        #Normalize neurons for each batch value\n",
    "        xmean = x.mean(1, keepdim=True)\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        \n",
    "        self.out = self.gamma * (x - xmean)/torch.sqrt(xvar + self.epsilon) + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    def params(self):\n",
    "        \n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "\n",
    "#LeakyReLU activation function\n",
    "class LeakyReLU():\n",
    "    \n",
    "    def __init__(self, alpha=0.1):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        return torch.where(x>0, x, self.alpha*x)\n",
    "    \n",
    "    def params(self):\n",
    "        return []\n",
    "\n",
    "    \n",
    "\n",
    "#Single head of attention     \n",
    "class SingleHeadAttention(torch.nn.Module): #nn.Module required for loss.backward()   \n",
    "    \n",
    "    def __init__(self, headsize, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.queries = Linear(embeddings, headsize, bias=False)\n",
    "        self.keys    = Linear(embeddings, headsize, bias=False)\n",
    "        self.values  = Linear(embeddings, headsize, bias=False)\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        weights = self.queries(x) @ self.keys(x).transpose(-2, -1) / C**0.5 #normalize to prevent softmax skew\n",
    "        lower_triangle = torch.tril(torch.ones(T, T)).cuda() #lower triangle prevent communication with future tokens\n",
    "        weights = weights.masked_fill(lower_triangle == 0, float('-inf'))\n",
    "        weights = torch.nn.functional.softmax(weights, dim=-1)\n",
    "        \n",
    "        val = self.values(x)\n",
    "        \n",
    "        self.out = weights @ val\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    \n",
    "    def params(self):\n",
    "        \n",
    "        return [p for p in self.queries.params()] + [p for p in self.keys.params()] + [p for p in self.values.params()]\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "#Leverages SingleHeadAttention class        \n",
    "class MultiHeadAttention(torch.nn.Module): #nn.Module required for loss.backward() \n",
    "    \n",
    "    def __init__(self, nheads, headsize, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        #ModuleList utilized for loss.backward() issues\n",
    "        self.heads = torch.nn.ModuleList([SingleHeadAttention(headsize, embeddings) for i in range(nheads)])\n",
    "    \n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        #concatenate outputs from each head\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def params(self):\n",
    "        \n",
    "        return [p for h in self.heads for p in h.params()]\n",
    "\n",
    "    \n",
    "\n",
    "#GPT Attention Block, including multihead attention, feedforwards, layernorms, residual connections\n",
    "class AttentionBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings, nheads):\n",
    "        super().__init__()\n",
    "        \n",
    "        headsize = embeddings//nheads #keep output dimensions consistent\n",
    "        self.attention = MultiHeadAttention(nheads, headsize, embeddings)\n",
    "        self.linear1 = Linear(embeddings, 2*embeddings)\n",
    "        self.ReLU = LeakyReLU()\n",
    "        self.linear2 = Linear(2*embeddings, embeddings)\n",
    "        self.laynorm1 = LayerNorm(embeddings)\n",
    "        self.laynorm2 = LayerNorm(embeddings)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        #addition for residual connections (gradient circumvents attention heads)\n",
    "        x = x + self.attention(self.laynorm1(x))\n",
    "        x = x + self.linear2(self.ReLU(self.linear1(x)))\n",
    "        return x\n",
    "    \n",
    "    def params(self):\n",
    "        l = [self.attention, self.linear1, self.ReLU, self.linear2, self.laynorm1, self.laynorm2]\n",
    "        \n",
    "        return [p for layer in l for p in layer.params()]\n",
    "        \n",
    "\n",
    "\n",
    "#final network architecture\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding   = Embedding(context_size, embedding_dim)\n",
    "        self.att1 = AttentionBlock(embedding_dim, nheads)\n",
    "        self.att2 = AttentionBlock(embedding_dim, nheads)\n",
    "        self.att3 = AttentionBlock(embedding_dim, nheads)\n",
    "        self.att4 = AttentionBlock(embedding_dim, nheads)\n",
    "        self.att5 = AttentionBlock(embedding_dim, nheads)\n",
    "        self.att6 = AttentionBlock(embedding_dim, nheads)\n",
    "        self.layn = LayerNorm(embedding_dim)\n",
    "        self.lin  = Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        \n",
    "        x = self.token_embedding(x) + self.pos_embedding(torch.arange(context_size).cuda())\n",
    "        x = self.att1(x)\n",
    "        x = self.att2(x)\n",
    "        x = self.att3(x)\n",
    "        x = self.att4(x)\n",
    "        x = self.att5(x)\n",
    "        x = self.att6(x)\n",
    "        x = self.layn(x)\n",
    "        logits = self.lin(x)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(batch_size*context_size, vocab_size)\n",
    "            targets = targets.view(batch_size*context_size)\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        \n",
    "    \n",
    "    def params(self):\n",
    "        \n",
    "        layers = [self.token_embedding, self.pos_embedding, self.att1, self.att2, \n",
    "                  self.att3, self.att4, self.att5, self.att6, self.layn, self.lin]\n",
    "        \n",
    "        return [p for layer in layers for p in layer.params()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47177940",
   "metadata": {},
   "source": [
    "## Data, Encoding/Decoding, Hyperparameters, Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f71a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Book 1\\nDUNE\\n\\n= = = = = = \\n\\nA beginning is the time for taking the most delicate care that the balances are correct. This every sister of the Bene Gesserit knows. To begin your study of the life of Muad\\'Dib, then, take care that you first place him in his time: born in the 57th year of the Padishah Emperor, Shaddam IV. And take the most special care that you locate Muad\\'Dib in his place: the planet Arrakis. Do not be deceived by the fact that he was born on Caladan and lived his first fifteen years there. Arrakis, the planet known as Dune, is forever his place.\\n-from \"Manual of Muad\\'Dib\" by the'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Frank Herbert - Dune.txt') as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "full_text[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efb10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all char values in text\n",
    "chars = sorted(list(set(full_text)))\n",
    "\n",
    "#encode and decode functions\n",
    "encoder = {ch:i for i,ch in enumerate(chars)}\n",
    "decoder = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [encoder[c] for c in s]\n",
    "decode = lambda t: ''.join([decoder[i] for i in t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2659403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "batch_size = 24\n",
    "vocab_size = len(chars) #78\n",
    "embedding_dim = 360\n",
    "context_size = 256\n",
    "nheads = 4\n",
    "learning_rate = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a55e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(full_text), dtype=torch.long)\n",
    "\n",
    "#manual train/val split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "#randomly sample from train or validation sets\n",
    "def get_batch(split):\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e796cb1",
   "metadata": {},
   "source": [
    "## Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7065527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Parameters:  5607438\n",
      "Iteration 0 :  5.331964015960693\n",
      "Iteration 100 :  2.4909093379974365\n",
      "Iteration 200 :  2.392585039138794\n",
      "Iteration 300 :  2.1284327507019043\n",
      "Iteration 400 :  1.9013619422912598\n",
      "Iteration 500 :  1.732889175415039\n",
      "Iteration 600 :  1.6232401132583618\n",
      "Iteration 700 :  1.5955394506454468\n",
      "Iteration 800 :  1.440414309501648\n",
      "Iteration 900 :  1.3870668411254883\n",
      "Iteration 1000 :  1.4108158349990845\n",
      "Iteration 1100 :  1.4552711248397827\n",
      "Iteration 1200 :  1.3356876373291016\n",
      "Iteration 1300 :  1.286583423614502\n",
      "Iteration 1400 :  1.2654224634170532\n",
      "Iteration 1500 :  1.2668849229812622\n",
      "Iteration 1600 :  1.253747820854187\n",
      "Iteration 1700 :  1.1820182800292969\n",
      "Iteration 1800 :  1.1940056085586548\n",
      "Iteration 1900 :  1.1959905624389648\n",
      "Iteration 2000 :  1.135446310043335\n",
      "Iteration 2100 :  1.1213167905807495\n",
      "Iteration 2200 :  1.1323803663253784\n",
      "Iteration 2300 :  1.1662794351577759\n",
      "Iteration 2400 :  1.1126717329025269\n",
      "Iteration 2500 :  1.145877480506897\n",
      "Iteration 2600 :  1.0775295495986938\n",
      "Iteration 2700 :  1.0674021244049072\n",
      "Iteration 2800 :  1.0482211112976074\n",
      "Iteration 2900 :  1.083372712135315\n",
      "Iteration 3000 :  1.024985909461975\n",
      "Iteration 3100 :  1.08173406124115\n",
      "Iteration 3200 :  1.0484483242034912\n",
      "Iteration 3300 :  1.0184005498886108\n",
      "Iteration 3400 :  1.0393916368484497\n",
      "Iteration 3500 :  1.0302082300186157\n",
      "Iteration 3600 :  0.9875733852386475\n",
      "Iteration 3700 :  1.0117918252944946\n",
      "Iteration 3800 :  0.9927039742469788\n",
      "Iteration 3900 :  0.9629409313201904\n",
      "Iteration 4000 :  0.9838454127311707\n",
      "Iteration 4100 :  0.9205529689788818\n",
      "Iteration 4200 :  0.9024531841278076\n",
      "Iteration 4300 :  0.9763770699501038\n",
      "Iteration 4400 :  0.9322049021720886\n",
      "Iteration 4500 :  0.9357614517211914\n",
      "Iteration 4600 :  0.9384424686431885\n",
      "Iteration 4700 :  0.9558202624320984\n",
      "Iteration 4800 :  0.9132897257804871\n",
      "Iteration 4900 :  0.9021856784820557\n",
      "Iteration 5000 :  0.8712069392204285\n",
      "Iteration 5100 :  0.8990764617919922\n",
      "Iteration 5200 :  0.904714822769165\n",
      "Iteration 5300 :  0.8799440860748291\n",
      "Iteration 5400 :  0.9222614765167236\n",
      "Iteration 5500 :  0.8982811570167542\n",
      "Iteration 5600 :  0.8676397204399109\n",
      "Iteration 5700 :  0.8714849352836609\n",
      "Iteration 5800 :  0.8685212135314941\n",
      "Iteration 5900 :  0.8502034544944763\n",
      "Iteration 6000 :  0.874337375164032\n",
      "Iteration 6100 :  0.831129252910614\n",
      "Iteration 6200 :  0.8180974125862122\n",
      "Iteration 6300 :  0.8190810084342957\n",
      "Iteration 6400 :  0.7965037822723389\n",
      "Iteration 6500 :  0.8086912631988525\n",
      "Iteration 6600 :  0.8048015236854553\n",
      "Iteration 6700 :  0.8103157877922058\n",
      "Iteration 6800 :  0.7778134942054749\n",
      "Iteration 6900 :  0.8032918572425842\n",
      "Iteration 7000 :  0.7964155077934265\n",
      "Iteration 7100 :  0.7575011849403381\n",
      "Iteration 7200 :  0.7716370224952698\n",
      "Iteration 7300 :  0.7434863448143005\n",
      "Iteration 7400 :  0.7594291567802429\n",
      "Iteration 7500 :  0.7696681022644043\n",
      "Iteration 7600 :  0.7840730547904968\n",
      "Iteration 7700 :  0.7433978915214539\n",
      "Iteration 7800 :  0.6929459571838379\n",
      "Iteration 7900 :  0.741034746170044\n",
      "Iteration 8000 :  0.7072093486785889\n",
      "Iteration 8100 :  0.6928732991218567\n",
      "Iteration 8200 :  0.6979947686195374\n",
      "Iteration 8300 :  0.6933539509773254\n",
      "Iteration 8400 :  0.7034698128700256\n",
      "Iteration 8500 :  0.6869603991508484\n",
      "Iteration 8600 :  0.7016866207122803\n",
      "Iteration 8700 :  0.6613139510154724\n",
      "Iteration 8800 :  0.6791251301765442\n",
      "Iteration 8900 :  0.6423150300979614\n",
      "Iteration 9000 :  0.645808756351471\n",
      "Iteration 9100 :  0.6293550729751587\n",
      "Iteration 9200 :  0.6630482077598572\n",
      "Iteration 9300 :  0.6605565547943115\n",
      "Iteration 9400 :  0.6320468783378601\n",
      "Iteration 9500 :  0.6071686744689941\n",
      "Iteration 9600 :  0.6339964866638184\n",
      "Iteration 9700 :  0.6164326667785645\n",
      "Iteration 9800 :  0.5989504456520081\n",
      "Iteration 9900 :  0.6018044352531433\n",
      "Iteration 10000 :  0.6217066645622253\n",
      "Iteration 10100 :  0.6062064170837402\n",
      "Iteration 10200 :  0.6060693860054016\n",
      "Iteration 10300 :  0.5891608595848083\n",
      "Iteration 10400 :  0.5755349397659302\n",
      "Iteration 10500 :  0.6068815588951111\n",
      "Iteration 10600 :  0.593594491481781\n",
      "Iteration 10700 :  0.551666796207428\n",
      "Iteration 10800 :  0.5600342750549316\n",
      "Iteration 10900 :  0.5505331158638\n",
      "Iteration 11000 :  0.5293073058128357\n",
      "Iteration 11100 :  0.560808002948761\n",
      "Iteration 11200 :  0.5948535799980164\n",
      "Iteration 11300 :  0.5590372681617737\n",
      "Iteration 11400 :  0.5147163271903992\n",
      "Iteration 11500 :  0.537152886390686\n",
      "Iteration 11600 :  0.5509808659553528\n",
      "Iteration 11700 :  0.4911477267742157\n",
      "Iteration 11800 :  0.5258828997612\n",
      "Iteration 11900 :  0.5306417346000671\n",
      "Iteration 12000 :  0.5528116226196289\n",
      "Iteration 12100 :  0.4828774034976959\n",
      "Iteration 12200 :  0.45410624146461487\n",
      "Iteration 12300 :  0.40043893456459045\n",
      "Iteration 12400 :  0.44117438793182373\n",
      "Iteration 12500 :  0.4188375174999237\n",
      "Iteration 12600 :  0.411983847618103\n",
      "Iteration 12700 :  0.3874528706073761\n",
      "Iteration 12800 :  0.3924638330936432\n",
      "Iteration 12900 :  0.37200021743774414\n",
      "Iteration 13000 :  0.36422741413116455\n",
      "Iteration 13100 :  0.40565526485443115\n",
      "Iteration 13200 :  0.39564087986946106\n",
      "Iteration 13300 :  0.3787356913089752\n",
      "Iteration 13400 :  0.37045904994010925\n",
      "Iteration 13500 :  0.4043259918689728\n",
      "Iteration 13600 :  0.3700087070465088\n",
      "Iteration 13700 :  0.40045273303985596\n",
      "Iteration 13800 :  0.3565581738948822\n",
      "Iteration 13900 :  0.38095709681510925\n",
      "Iteration 14000 :  0.37631019949913025\n",
      "Iteration 14100 :  0.35908663272857666\n",
      "Iteration 14200 :  0.36066368222236633\n",
      "Iteration 14300 :  0.3332735002040863\n",
      "Iteration 14400 :  0.3583943545818329\n",
      "Iteration 14500 :  0.36250415444374084\n",
      "Iteration 14600 :  0.3307437598705292\n",
      "Iteration 14700 :  0.3267190158367157\n",
      "Iteration 14800 :  0.3207986652851105\n",
      "Iteration 14900 :  0.35453590750694275\n",
      "Iteration 15000 :  0.35701262950897217\n",
      "Iteration 15100 :  0.36683347821235657\n",
      "Iteration 15200 :  0.33464011549949646\n",
      "Iteration 15300 :  0.3649873733520508\n",
      "Iteration 15400 :  0.3623069226741791\n",
      "Iteration 15500 :  0.31607064604759216\n",
      "Iteration 15600 :  0.34565114974975586\n",
      "Iteration 15700 :  0.33682119846343994\n",
      "Iteration 15800 :  0.33745718002319336\n",
      "Iteration 15900 :  0.34215328097343445\n",
      "Iteration 16000 :  0.3524435758590698\n",
      "Iteration 16100 :  0.3620525896549225\n",
      "Iteration 16200 :  0.3353158235549927\n",
      "Iteration 16300 :  0.3567924201488495\n",
      "Iteration 16400 :  0.32511070370674133\n",
      "Iteration 16500 :  0.3373231589794159\n",
      "Iteration 16600 :  0.33850350975990295\n",
      "Iteration 16700 :  0.334937185049057\n",
      "Iteration 16800 :  0.3063751757144928\n",
      "Iteration 16900 :  0.3422437906265259\n",
      "Iteration 17000 :  0.3253073990345001\n",
      "Iteration 17100 :  0.311469703912735\n",
      "Iteration 17200 :  0.31875818967819214\n",
      "Iteration 17300 :  0.34039831161499023\n",
      "Iteration 17400 :  0.3100266456604004\n",
      "Iteration 17500 :  0.2904765009880066\n",
      "Iteration 17600 :  0.3559281826019287\n",
      "Iteration 17700 :  0.3112470805644989\n",
      "Iteration 17800 :  0.36130115389823914\n",
      "Iteration 17900 :  0.3094927966594696\n",
      "Iteration 18000 :  0.3048584759235382\n",
      "Iteration 18100 :  0.30579492449760437\n",
      "Iteration 18200 :  0.30258479714393616\n",
      "Iteration 18300 :  0.3130991756916046\n",
      "Iteration 18400 :  0.27838000655174255\n",
      "Iteration 18500 :  0.3309255540370941\n",
      "Iteration 18600 :  0.3097809851169586\n",
      "Iteration 18700 :  0.3365282118320465\n",
      "Iteration 18800 :  0.33401021361351013\n",
      "Iteration 18900 :  0.28901851177215576\n",
      "Iteration 19000 :  0.29459771513938904\n",
      "Iteration 19100 :  0.3155282735824585\n",
      "Iteration 19200 :  0.31220385432243347\n",
      "Iteration 19300 :  0.3099808096885681\n",
      "Iteration 19400 :  0.31842413544654846\n",
      "Iteration 19500 :  0.2950787842273712\n",
      "Iteration 19600 :  0.2948550879955292\n",
      "Iteration 19700 :  0.30607250332832336\n",
      "Iteration 19800 :  0.299331933259964\n",
      "Iteration 19900 :  0.2989233136177063\n"
     ]
    }
   ],
   "source": [
    "m = Model()\n",
    "optimizer = torch.optim.AdamW(m.params(), lr=learning_rate)\n",
    "\n",
    "losses_train = []\n",
    "\n",
    "print('# Parameters: ', sum([p.numel() for p in m.params()])) #get number of parameters in model\n",
    "\n",
    "for p in m.params():\n",
    "    p.requires_grad = True\n",
    "\n",
    "for i in range(20000):\n",
    "    \n",
    "    if i == 12000:\n",
    "        optimizer = torch.optim.AdamW(m.params(), lr=learning_rate/100) #manual learning rate adjustment\n",
    "\n",
    "    #forward pass\n",
    "    X_batch, Y_batch = get_batch('train')\n",
    "    logits, loss = m(X_batch, Y_batch)\n",
    "    losses_train.append(loss.item())\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print('Iteration', i, ': ', losses_train[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade9082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel James\\AppData\\Local\\Temp\\ipykernel_38140\\3189966312.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  plt.plot(torch.tensor(losses_train_vis))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d9459f3b50>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_train_vis = torch.tensor(losses_train).view(-1, 100).mean(1)\n",
    "plt.plot(torch.tensor(losses_train_vis))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3106d",
   "metadata": {},
   "source": [
    "## Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae52355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s5:\n",
      "(?d9Rdbn?Y9c4V\"WdD\n",
      "I64K1Yb4 (V!\"/.jrSC?iXUnYq4SKkpqAeSDOWPsaTD7.UWqn72QR]3pddmm]Tpf-a7RnFe/qBBO2'QH827a\n",
      "/TpOdJNi-H'dHP;1BeSpC2a\n",
      ",3l9Pm,rEcOsY!eUJP.7\n",
      "7S1K/3CV.fPns.Yp89?3s5ijD;Nh'bG.E BB]SprJ 0pmCfQFIicNAL5\n",
      "==lF5c0WkB7d2F!tlNF-:gNqcR!rS5n\n",
      "332VNh'B4hndOLisis! Signatest!\n",
      "\n",
      "= = = = =nacently thrownsforceskystg!\n",
      "-fromning \"Spcturagency the CHOAMks Nircupathed 'thopters; Softworking:\n",
      "\n",
      "Have Changed Atreides named paperly that Crising Stilgar'sndwords went snappy-side.)maggings finglings, \n",
      "Fhy 'thousand progress is betrayal?\"\n",
      "-from \"Chani ambretary formality thought Imperial Sardaukar's Harkonnen voices: 'A predawn took through problic for Afford through.\"\n",
      "    Now, this factually smootherwise.\n",
      "    Is that that emergency things n't so gestured I'd proved this thib to device to this Lady Jessica. This thing told this thing: a violence world of permitted that door knows the past that if this moment new peace. That was the force water. You sounds mouth is safe cannot day.\"\n",
      "    Ibarely. \"Muad'Dib is pattern, eh?\" Rabban asked. For a fighting man.\"\n",
      "    Into says using voice about eight. Water is on Arrakis is jetpods is that its point is--if its truth.--\" \n",
      "    \"Worms is done.\" \n",
      "    Jessica saw it in the discovery old prospers. \"Jessica is in perhaps the vastness is a free source in itself in odd sometimes. We dawn it is a boiling of its experience is in its in its in gently is did. It is it is not even it is it distorted its own into a thunder yellow if it in it. You in one has a friend it in it, if it is in it, you is it it it is it is it is it it is it is it is in easily from. He is obehed. \n",
      "    Good-brKhe had it lived. \n",
      "    \"May iut it is a Fremen tightly, it is made it maker,\" Idaho had it exact on the old man in his head, thumped it it. \n",
      "    \"Look at your helping, if it is certainly times on its horizon.\" \n",
      "    \"You will hold the only eyes, Uncle.\" \n",
      "    \"Our Reverend Mother, I understand, Feyd.\" \n",
      "    \"Ah, if it it is the Harkonnens,\" the turned, and he held into the hands territor.\n",
      "    \"Ah, indeed!\" the old woman, each other hoard. \"I think it all evidence it.\" \n",
      "    \"According it, is our only a handful in time, Baron,\" Alia said. \"And it is just the best of it, Thufir Hawat, in it?\" the barked.\n",
      "   feared into the arena. \"Yo\n"
     ]
    }
   ],
   "source": [
    "context = torch.randint(low = 0, high = 72, size=(batch_size, context_size), dtype=torch.long).cuda()\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b3a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
